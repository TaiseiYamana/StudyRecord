{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaiseiYamana/StudyRecord/blob/main/MobileVit_training_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "pytorch cifar100 data auguments: \n",
        "https://github.com/weiaicunzai/pytorch-cifar100\n",
        "\n",
        "MobilVitConfig argument: \n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig"
      ],
      "metadata": {
        "id": "Rqa1Jllvb64_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuL_64MBppg9",
        "outputId": "0a9e6574-445c-4909-bced-4a3304e4c3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MobileViTConfig, MobileViTForImageClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR\n",
        "\n",
        "image_size  = 32\n",
        "patch_size = 8 # image_sizeの約数であること\n",
        "n_classes = 100\n",
        "seed = 1\n",
        "\n",
        "lr = 1e-2\n",
        "momentum = 0.9\n",
        "weight_decay = 0.01\n",
        "batch_size = 64\n",
        "\n",
        "# 乱数生成シード値固定設定\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(0)\n",
        "cudnn.deterministic = False\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# MobileViTセットアップ\n",
        "configuration = MobileViTConfig(image_size = image_size, patch_size = patch_size)\n",
        "model = MobileViTForImageClassification(configuration).from_pretrained(\"apple/mobilevit-small\")\n",
        "#model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n",
        "model.classifier = nn.Linear(640, n_classes)\n",
        "#torch.nn.init.normal_(model.classifier.weight, mean=0, std=1)\n",
        "#model.classifier.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "iMt9V5iUfQ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiEw2Ya30Vkl",
        "outputId": "4b23c3cc-765d-46c1-e11c-69a2d01a8991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5001732"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
        "std = [0.2673342858792401, 0.2564384629170883, 0.27615047132568404]\n",
        "\n",
        "test_mean = [0.5088964127604166, 0.48739301317401956, 0.44194221124387256]\n",
        "test_std = [0.2682515741720801, 0.2573637364478126, 0.2770957707973042]\n",
        "\n",
        "class RGB_2_BGR():\n",
        "  def __call__(self, img):\n",
        "    r, g, b = img.split()\n",
        "    return Image.merge(\"RGB\", (b, g, r))\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "            #RGB_2_BGR(),\n",
        "            transforms.RandomCrop(image_size, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "            #RGB_2_BGR(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(test_mean, test_std)])\n",
        "\n",
        "train_dataset = CIFAR100(root = './', train = True, transform = train_transform, download = True)\n",
        "testTrain_dataset = CIFAR100(root = './', train = True, transform = test_transform, download = True)\n",
        "test_dataset = CIFAR100(root = './', train = False, transform = test_transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "testTrain_loader = DataLoader(testTrain_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader_dict = {'Train': testTrain_loader, 'Test': test_loader}"
      ],
      "metadata": {
        "id": "1KJRxPa-vyr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76fd39b-f5c3-4747-ed81-0372027bb9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = []\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params.append(param)\n",
        "\n",
        "#optimizer = optim.SGD([\n",
        "#        {'params':  params[:-2], 'lr':1.0*lr},\n",
        "#        {'params':  params[-1], 'lr':10.0*lr}\n",
        "#    ], lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "        {'params':  params[:-2], 'lr':1.0*lr},\n",
        "        {'params':  params[-1], 'lr':10.0*lr}\n",
        "        ], lr = lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "#scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=80, eta_min=0.001)"
      ],
      "metadata": {
        "id": "N9HtcvqESbDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_combination(x, y, epsilon):\n",
        "    return (1 - epsilon) * x + epsilon * y\n",
        "\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, epsilon=0.1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, preds, target):\n",
        "        n = preds.size()[-1]\n",
        "        log_preds = F.log_softmax(preds, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
        "        return linear_combination(nll, loss/n, self.epsilon)\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "  model.train()\n",
        "\n",
        "  for ite, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs).logits\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, test_loader_dict, phase, device):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader_dict[phase]:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs).logits\n",
        "      test_loss += F.cross_entropy(outputs, labels, size_average=False).item()\n",
        "\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      correct += torch.sum(preds == labels.data)\n",
        "      \n",
        "    test_loss /= len(test_loader_dict[phase].dataset)\n",
        "    test_acc = correct / len(test_loader_dict[phase].dataset)\n",
        "\n",
        "    print(f'{phase} loss : {test_loss:.4f} acc : {test_acc:.4f}')\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "VHKZlHYE1UuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUが使用できるかを確認\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "history = {}\n",
        "history['train_loss'] = []\n",
        "history['train_acc'] = []\n",
        "history['test_loss'] = []\n",
        "history['test_acc'] = []\n",
        "\n",
        "num_epochs = 240\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    print('-----------------------')\n",
        "    print('Epoch {}/{}, lr: {}'.format(epoch,num_epochs, optimizer.param_groups[0][\"lr\"]))\n",
        "\n",
        "    train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = test(model, test_loader_dict, 'Train', device)\n",
        "    test_loss, test_acc = test(model, test_loader_dict, 'Test', device)\n",
        "\n",
        "    scheduler.step() \n",
        "\n",
        "    history['train_loss'].append(val_loss)\n",
        "    history['train_acc'].append(val_acc.to('cpu').item())\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc.to('cpu').item())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7p14yneDuafj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba4f062-579e-44d9-ff1e-a95f3ae11d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "Epoch 1/240, lr: 0.01\n",
            "Train loss : 3.7450 acc : 0.1188\n",
            "Test loss : 3.7637 acc : 0.1186\n",
            "-----------------------\n",
            "Epoch 2/240, lr: 0.009996530663083255\n",
            "Train loss : 3.3970 acc : 0.1667\n",
            "Test loss : 3.4175 acc : 0.1626\n",
            "-----------------------\n",
            "Epoch 3/240, lr: 0.009986128001799075\n",
            "Train loss : 3.4293 acc : 0.1747\n",
            "Test loss : 3.4719 acc : 0.1710\n",
            "-----------------------\n",
            "Epoch 4/240, lr: 0.009968808056297167\n",
            "Train loss : 2.9056 acc : 0.2616\n",
            "Test loss : 2.9526 acc : 0.2505\n",
            "-----------------------\n",
            "Epoch 5/240, lr: 0.009944597532678116\n",
            "Train loss : 2.7961 acc : 0.2887\n",
            "Test loss : 2.8647 acc : 0.2751\n",
            "-----------------------\n",
            "Epoch 6/240, lr: 0.009913533761814534\n",
            "Train loss : 2.6715 acc : 0.3130\n",
            "Test loss : 2.7441 acc : 0.2958\n",
            "-----------------------\n",
            "Epoch 7/240, lr: 0.009875664641789541\n",
            "Train loss : 2.5863 acc : 0.3345\n",
            "Test loss : 2.6634 acc : 0.3146\n",
            "-----------------------\n",
            "Epoch 8/240, lr: 0.00983104856404141\n",
            "Train loss : 2.5761 acc : 0.3352\n",
            "Test loss : 2.6636 acc : 0.3181\n",
            "-----------------------\n",
            "Epoch 9/240, lr: 0.009779754323328189\n",
            "Train loss : 2.3779 acc : 0.3773\n",
            "Test loss : 2.4755 acc : 0.3565\n",
            "-----------------------\n",
            "Epoch 10/240, lr: 0.009721861011651175\n",
            "Train loss : 2.4004 acc : 0.3749\n",
            "Test loss : 2.5110 acc : 0.3503\n",
            "-----------------------\n",
            "Epoch 11/240, lr: 0.009657457896300785\n",
            "Train loss : 2.2917 acc : 0.3974\n",
            "Test loss : 2.3932 acc : 0.3785\n",
            "-----------------------\n",
            "Epoch 12/240, lr: 0.009586644282212859\n",
            "Train loss : 2.2099 acc : 0.4143\n",
            "Test loss : 2.3318 acc : 0.3907\n",
            "-----------------------\n",
            "Epoch 13/240, lr: 0.009509529358847649\n",
            "Train loss : 2.2091 acc : 0.4183\n",
            "Test loss : 2.3203 acc : 0.3938\n",
            "-----------------------\n",
            "Epoch 14/240, lr: 0.009426232031827582\n",
            "Train loss : 2.1928 acc : 0.4247\n",
            "Test loss : 2.3177 acc : 0.3984\n",
            "-----------------------\n",
            "Epoch 15/240, lr: 0.00933688073959341\n",
            "Train loss : 2.2987 acc : 0.4015\n",
            "Test loss : 2.4146 acc : 0.3770\n",
            "-----------------------\n",
            "Epoch 16/240, lr: 0.00924161325536145\n",
            "Train loss : 2.0869 acc : 0.4502\n",
            "Test loss : 2.2197 acc : 0.4224\n",
            "-----------------------\n",
            "Epoch 17/240, lr: 0.00914057647468726\n",
            "Train loss : 2.1129 acc : 0.4438\n",
            "Test loss : 2.2341 acc : 0.4173\n",
            "-----------------------\n",
            "Epoch 18/240, lr: 0.009033926188963346\n",
            "Train loss : 1.9839 acc : 0.4759\n",
            "Test loss : 2.1076 acc : 0.4519\n",
            "-----------------------\n",
            "Epoch 19/240, lr: 0.008921826845200133\n",
            "Train loss : 1.9787 acc : 0.4722\n",
            "Test loss : 2.1329 acc : 0.4416\n",
            "-----------------------\n",
            "Epoch 20/240, lr: 0.008804451292460578\n",
            "Train loss : 2.0394 acc : 0.4658\n",
            "Test loss : 2.1821 acc : 0.4398\n",
            "-----------------------\n",
            "Epoch 21/240, lr: 0.008681980515339457\n",
            "Train loss : 1.9261 acc : 0.4850\n",
            "Test loss : 2.0920 acc : 0.4507\n",
            "-----------------------\n",
            "Epoch 22/240, lr: 0.008554603354898233\n",
            "Train loss : 1.9765 acc : 0.4789\n",
            "Test loss : 2.1440 acc : 0.4417\n",
            "-----------------------\n",
            "Epoch 23/240, lr: 0.008422516217485822\n",
            "Train loss : 1.8566 acc : 0.5037\n",
            "Test loss : 2.0293 acc : 0.4649\n",
            "-----------------------\n",
            "Epoch 24/240, lr: 0.008285922771894248\n",
            "Train loss : 1.8796 acc : 0.5073\n",
            "Test loss : 2.0454 acc : 0.4677\n",
            "-----------------------\n",
            "Epoch 25/240, lr: 0.008145033635316124\n",
            "Train loss : 1.8063 acc : 0.5192\n",
            "Test loss : 1.9838 acc : 0.4795\n",
            "-----------------------\n",
            "Epoch 26/240, lr: 0.008000066048588205\n",
            "Train loss : 1.8253 acc : 0.5123\n",
            "Test loss : 2.0247 acc : 0.4677\n",
            "-----------------------\n",
            "Epoch 27/240, lr: 0.007851243541221764\n",
            "Train loss : 1.7785 acc : 0.5281\n",
            "Test loss : 1.9508 acc : 0.4932\n",
            "-----------------------\n",
            "Epoch 28/240, lr: 0.007698795586736293\n",
            "Train loss : 1.8186 acc : 0.5175\n",
            "Test loss : 2.0119 acc : 0.4698\n",
            "-----------------------\n",
            "Epoch 29/240, lr: 0.007542957248827956\n",
            "Train loss : 1.7392 acc : 0.5406\n",
            "Test loss : 1.9440 acc : 0.4902\n",
            "-----------------------\n",
            "Epoch 30/240, lr: 0.007383968818918422\n",
            "Train loss : 1.7381 acc : 0.5396\n",
            "Test loss : 1.9569 acc : 0.4900\n",
            "-----------------------\n",
            "Epoch 31/240, lr: 0.0072220754456429\n",
            "Train loss : 1.7351 acc : 0.5392\n",
            "Test loss : 1.9536 acc : 0.4892\n",
            "-----------------------\n",
            "Epoch 32/240, lr: 0.0070575267568487145\n",
            "Train loss : 1.7290 acc : 0.5436\n",
            "Test loss : 1.9437 acc : 0.4949\n",
            "-----------------------\n",
            "Epoch 33/240, lr: 0.00689057647468726\n",
            "Train loss : 1.6693 acc : 0.5585\n",
            "Test loss : 1.8971 acc : 0.5028\n",
            "-----------------------\n",
            "Epoch 34/240, lr: 0.006721482024392831\n",
            "Train loss : 1.6134 acc : 0.5699\n",
            "Test loss : 1.8516 acc : 0.5147\n",
            "-----------------------\n",
            "Epoch 35/240, lr: 0.0065505041373515715\n",
            "Train loss : 1.5793 acc : 0.5796\n",
            "Test loss : 1.8319 acc : 0.5224\n",
            "-----------------------\n",
            "Epoch 36/240, lr: 0.006377906449072574\n",
            "Train loss : 1.5854 acc : 0.5785\n",
            "Test loss : 1.8482 acc : 0.5148\n",
            "-----------------------\n",
            "Epoch 37/240, lr: 0.006203955092681036\n",
            "Train loss : 1.5515 acc : 0.5907\n",
            "Test loss : 1.8070 acc : 0.5303\n",
            "-----------------------\n",
            "Epoch 38/240, lr: 0.006028918288560267\n",
            "Train loss : 1.5457 acc : 0.5835\n",
            "Test loss : 1.8087 acc : 0.5201\n",
            "-----------------------\n",
            "Epoch 39/240, lr: 0.0058530659307753\n",
            "Train loss : 1.5516 acc : 0.5884\n",
            "Test loss : 1.8364 acc : 0.5255\n",
            "-----------------------\n",
            "Epoch 40/240, lr: 0.005676669170915806\n",
            "Train loss : 1.4865 acc : 0.6058\n",
            "Test loss : 1.7476 acc : 0.5433\n",
            "-----------------------\n",
            "Epoch 41/240, lr: 0.005499999999999997\n",
            "Train loss : 1.4612 acc : 0.6104\n",
            "Test loss : 1.7561 acc : 0.5370\n",
            "-----------------------\n",
            "Epoch 42/240, lr: 0.00532333082908419\n",
            "Train loss : 1.4889 acc : 0.6051\n",
            "Test loss : 1.7728 acc : 0.5393\n",
            "-----------------------\n",
            "Epoch 43/240, lr: 0.005146934069224696\n",
            "Train loss : 1.4646 acc : 0.6050\n",
            "Test loss : 1.7622 acc : 0.5361\n",
            "-----------------------\n",
            "Epoch 44/240, lr: 0.004971081711439729\n",
            "Train loss : 1.4149 acc : 0.6278\n",
            "Test loss : 1.7157 acc : 0.5504\n",
            "-----------------------\n",
            "Epoch 45/240, lr: 0.004796044907318961\n",
            "Train loss : 1.4046 acc : 0.6294\n",
            "Test loss : 1.7130 acc : 0.5532\n",
            "-----------------------\n",
            "Epoch 46/240, lr: 0.004622093550927422\n",
            "Train loss : 1.4056 acc : 0.6264\n",
            "Test loss : 1.7177 acc : 0.5514\n",
            "-----------------------\n",
            "Epoch 47/240, lr: 0.004449495862648424\n",
            "Train loss : 1.3683 acc : 0.6395\n",
            "Test loss : 1.6948 acc : 0.5544\n",
            "-----------------------\n",
            "Epoch 48/240, lr: 0.0042785179756071635\n",
            "Train loss : 1.3365 acc : 0.6486\n",
            "Test loss : 1.6725 acc : 0.5615\n",
            "-----------------------\n",
            "Epoch 49/240, lr: 0.004109423525312735\n",
            "Train loss : 1.3154 acc : 0.6523\n",
            "Test loss : 1.6471 acc : 0.5696\n",
            "-----------------------\n",
            "Epoch 50/240, lr: 0.0039424732431512805\n",
            "Train loss : 1.2887 acc : 0.6600\n",
            "Test loss : 1.6575 acc : 0.5695\n",
            "-----------------------\n",
            "Epoch 51/240, lr: 0.0037779245543570943\n",
            "Train loss : 1.3061 acc : 0.6531\n",
            "Test loss : 1.6697 acc : 0.5684\n",
            "-----------------------\n",
            "Epoch 52/240, lr: 0.0036160311810815734\n",
            "Train loss : 1.2536 acc : 0.6706\n",
            "Test loss : 1.6233 acc : 0.5763\n",
            "-----------------------\n",
            "Epoch 53/240, lr: 0.003457042751172038\n",
            "Train loss : 1.2573 acc : 0.6694\n",
            "Test loss : 1.6280 acc : 0.5751\n",
            "-----------------------\n",
            "Epoch 54/240, lr: 0.003301204413263701\n",
            "Train loss : 1.2262 acc : 0.6755\n",
            "Test loss : 1.6119 acc : 0.5782\n",
            "-----------------------\n",
            "Epoch 55/240, lr: 0.0031487564587782293\n",
            "Train loss : 1.1956 acc : 0.6870\n",
            "Test loss : 1.6115 acc : 0.5850\n",
            "-----------------------\n",
            "Epoch 56/240, lr: 0.0029999339514117885\n",
            "Train loss : 1.2010 acc : 0.6851\n",
            "Test loss : 1.6147 acc : 0.5798\n",
            "-----------------------\n",
            "Epoch 57/240, lr: 0.0028549663646838704\n",
            "Train loss : 1.1387 acc : 0.6998\n",
            "Test loss : 1.5668 acc : 0.5917\n",
            "-----------------------\n",
            "Epoch 58/240, lr: 0.0027140772281057475\n",
            "Train loss : 1.1295 acc : 0.7042\n",
            "Test loss : 1.5605 acc : 0.5955\n",
            "-----------------------\n",
            "Epoch 59/240, lr: 0.0025774837825141735\n",
            "Train loss : 1.1208 acc : 0.7077\n",
            "Test loss : 1.5697 acc : 0.5906\n",
            "-----------------------\n",
            "Epoch 60/240, lr: 0.0024453966451017627\n",
            "Train loss : 1.1072 acc : 0.7113\n",
            "Test loss : 1.5547 acc : 0.5910\n",
            "-----------------------\n",
            "Epoch 61/240, lr: 0.002318019484660536\n",
            "Train loss : 1.0753 acc : 0.7201\n",
            "Test loss : 1.5491 acc : 0.5975\n",
            "-----------------------\n",
            "Epoch 62/240, lr: 0.0021955487075394136\n",
            "Train loss : 1.0597 acc : 0.7261\n",
            "Test loss : 1.5384 acc : 0.5992\n",
            "-----------------------\n",
            "Epoch 63/240, lr: 0.00207817315479986\n",
            "Train loss : 1.0441 acc : 0.7271\n",
            "Test loss : 1.5407 acc : 0.5955\n",
            "-----------------------\n",
            "Epoch 64/240, lr: 0.0019660738110366463\n",
            "Train loss : 1.0205 acc : 0.7365\n",
            "Test loss : 1.5222 acc : 0.6055\n",
            "-----------------------\n",
            "Epoch 65/240, lr: 0.0018594235253127363\n",
            "Train loss : 1.0076 acc : 0.7383\n",
            "Test loss : 1.5215 acc : 0.6021\n",
            "-----------------------\n",
            "Epoch 66/240, lr: 0.0017583867446385468\n",
            "Train loss : 0.9824 acc : 0.7494\n",
            "Test loss : 1.5022 acc : 0.6108\n",
            "-----------------------\n",
            "Epoch 67/240, lr: 0.0016631192604065845\n",
            "Train loss : 0.9924 acc : 0.7435\n",
            "Test loss : 1.5300 acc : 0.6019\n",
            "-----------------------\n",
            "Epoch 68/240, lr: 0.0015737679681724131\n",
            "Train loss : 0.9615 acc : 0.7526\n",
            "Test loss : 1.5113 acc : 0.6081\n",
            "-----------------------\n",
            "Epoch 69/240, lr: 0.0014904706411523446\n",
            "Train loss : 0.9474 acc : 0.7582\n",
            "Test loss : 1.4988 acc : 0.6120\n",
            "-----------------------\n",
            "Epoch 70/240, lr: 0.0014133557177871336\n",
            "Train loss : 0.9363 acc : 0.7620\n",
            "Test loss : 1.4913 acc : 0.6149\n",
            "-----------------------\n",
            "Epoch 71/240, lr: 0.0013425421036992093\n",
            "Train loss : 0.9034 acc : 0.7675\n",
            "Test loss : 1.4751 acc : 0.6157\n",
            "-----------------------\n",
            "Epoch 72/240, lr: 0.001278138988348821\n",
            "Train loss : 0.9109 acc : 0.7675\n",
            "Test loss : 1.4917 acc : 0.6111\n",
            "-----------------------\n",
            "Epoch 73/240, lr: 0.0012202456766718089\n",
            "Train loss : 0.8796 acc : 0.7737\n",
            "Test loss : 1.4725 acc : 0.6181\n",
            "-----------------------\n",
            "Epoch 74/240, lr: 0.0011689514359585875\n",
            "Train loss : 0.8701 acc : 0.7769\n",
            "Test loss : 1.4695 acc : 0.6224\n",
            "-----------------------\n",
            "Epoch 75/240, lr: 0.0011243353582104554\n",
            "Train loss : 0.8652 acc : 0.7803\n",
            "Test loss : 1.4691 acc : 0.6205\n",
            "-----------------------\n",
            "Epoch 76/240, lr: 0.001086466238185463\n",
            "Train loss : 0.8560 acc : 0.7858\n",
            "Test loss : 1.4744 acc : 0.6177\n",
            "-----------------------\n",
            "Epoch 77/240, lr: 0.0010554024673218804\n",
            "Train loss : 0.8520 acc : 0.7843\n",
            "Test loss : 1.4668 acc : 0.6222\n",
            "-----------------------\n",
            "Epoch 78/240, lr: 0.0010311919437028317\n",
            "Train loss : 0.8388 acc : 0.7890\n",
            "Test loss : 1.4707 acc : 0.6220\n",
            "-----------------------\n",
            "Epoch 79/240, lr: 0.0010138719982009242\n",
            "Train loss : 0.8277 acc : 0.7914\n",
            "Test loss : 1.4628 acc : 0.6230\n",
            "-----------------------\n",
            "Epoch 80/240, lr: 0.001003469336916747\n",
            "Train loss : 0.8250 acc : 0.7933\n",
            "Test loss : 1.4615 acc : 0.6245\n",
            "-----------------------\n",
            "Epoch 81/240, lr: 0.001\n",
            "Train loss : 0.8164 acc : 0.7969\n",
            "Test loss : 1.4656 acc : 0.6212\n",
            "-----------------------\n",
            "Epoch 82/240, lr: 0.001003469336916747\n",
            "Train loss : 0.8259 acc : 0.7926\n",
            "Test loss : 1.4807 acc : 0.6214\n",
            "-----------------------\n",
            "Epoch 83/240, lr: 0.0010138719982009238\n",
            "Train loss : 0.8102 acc : 0.7968\n",
            "Test loss : 1.4750 acc : 0.6209\n",
            "-----------------------\n",
            "Epoch 84/240, lr: 0.001031191943702832\n",
            "Train loss : 0.8193 acc : 0.7939\n",
            "Test loss : 1.4792 acc : 0.6195\n",
            "-----------------------\n",
            "Epoch 85/240, lr: 0.0010554024673218804\n",
            "Train loss : 0.8036 acc : 0.7989\n",
            "Test loss : 1.4729 acc : 0.6221\n",
            "-----------------------\n",
            "Epoch 86/240, lr: 0.0010864662381854637\n",
            "Train loss : 0.8285 acc : 0.7934\n",
            "Test loss : 1.5042 acc : 0.6162\n",
            "-----------------------\n",
            "Epoch 87/240, lr: 0.0011243353582104558\n",
            "Train loss : 0.8186 acc : 0.7966\n",
            "Test loss : 1.4917 acc : 0.6145\n",
            "-----------------------\n",
            "Epoch 88/240, lr: 0.001168951435958588\n",
            "Train loss : 0.8270 acc : 0.7940\n",
            "Test loss : 1.4974 acc : 0.6129\n",
            "-----------------------\n",
            "Epoch 89/240, lr: 0.0012202456766718097\n",
            "Train loss : 0.8341 acc : 0.7899\n",
            "Test loss : 1.5102 acc : 0.6122\n",
            "-----------------------\n",
            "Epoch 90/240, lr: 0.0012781389883488234\n",
            "Train loss : 0.8266 acc : 0.7932\n",
            "Test loss : 1.4896 acc : 0.6206\n",
            "-----------------------\n",
            "Epoch 91/240, lr: 0.0013425421036992117\n",
            "Train loss : 0.8147 acc : 0.7951\n",
            "Test loss : 1.4716 acc : 0.6212\n",
            "-----------------------\n",
            "Epoch 92/240, lr: 0.0014133557177871364\n",
            "Train loss : 0.8407 acc : 0.7902\n",
            "Test loss : 1.4974 acc : 0.6170\n",
            "-----------------------\n",
            "Epoch 93/240, lr: 0.001490470641152348\n",
            "Train loss : 0.8403 acc : 0.7884\n",
            "Test loss : 1.4834 acc : 0.6180\n",
            "-----------------------\n",
            "Epoch 94/240, lr: 0.0015737679681724164\n",
            "Train loss : 0.8810 acc : 0.7742\n",
            "Test loss : 1.5341 acc : 0.6085\n",
            "-----------------------\n",
            "Epoch 95/240, lr: 0.0016631192604065906\n",
            "Train loss : 0.8625 acc : 0.7813\n",
            "Test loss : 1.5212 acc : 0.6103\n",
            "-----------------------\n",
            "Epoch 96/240, lr: 0.001758386744638552\n",
            "Train loss : 0.8655 acc : 0.7811\n",
            "Test loss : 1.5032 acc : 0.6136\n",
            "-----------------------\n",
            "Epoch 97/240, lr: 0.0018594235253127423\n",
            "Train loss : 0.9019 acc : 0.7718\n",
            "Test loss : 1.5488 acc : 0.6088\n",
            "-----------------------\n",
            "Epoch 98/240, lr: 0.001966073811036654\n",
            "Train loss : 0.9320 acc : 0.7620\n",
            "Test loss : 1.5589 acc : 0.6043\n",
            "-----------------------\n",
            "Epoch 99/240, lr: 0.0020781731547998674\n",
            "Train loss : 0.9496 acc : 0.7572\n",
            "Test loss : 1.5700 acc : 0.5978\n",
            "-----------------------\n",
            "Epoch 100/240, lr: 0.002195548707539425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "\n",
        "def plot_graph(values1, values2, rng, label1, label2):\n",
        "    plt.plot(range(rng), values1, label=label1)\n",
        "    plt.plot(range(rng), values2, label=label2)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()  "
      ],
      "metadata": {
        "id": "Kyf6Aza4sRB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history['train_acc'], history['test_acc'], num_epochs,'train_acc', 'test_acc')"
      ],
      "metadata": {
        "id": "lMKX3Sc7sSAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history['train_loss'], history['test_loss'], num_epochs,'train_loss', 'test_loss')"
      ],
      "metadata": {
        "id": "gtp91Tt6VowB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}