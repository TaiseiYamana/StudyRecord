{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGu6x0na+4HxWTcAbOXTRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaiseiYamana/StudyRecord/blob/main/MobileVit_training_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "pytorch cifar100 data auguments: \n",
        "https://github.com/weiaicunzai/pytorch-cifar100\n",
        "\n",
        "MobilVitConfig argument: \n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig"
      ],
      "metadata": {
        "id": "Rqa1Jllvb64_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuL_64MBppg9",
        "outputId": "94648947-0d3d-4c86-bab7-c04ba099945b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MobileViTConfig, MobileViTForImageClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "image_size  = 64\n",
        "patch_size = 8 # image_sizeの約数であること\n",
        "n_classes = 100\n",
        "seed = 1\n",
        "\n",
        "lr = 1e-1\n",
        "momentum = 0.9\n",
        "batch_size = 32\n",
        "\n",
        "# 乱数生成シード値固定設定\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(0)\n",
        "cudnn.deterministic = False\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# MobileViTセットアップ\n",
        "configuration = MobileViTConfig(image_size = image_size, patch_size = 32)\n",
        "model = MobileViTForImageClassification(configuration).from_pretrained(\"apple/mobilevit-small\")\n",
        "model.classifier = nn.Linear(640, n_classes)\n",
        "#torch.nn.init.normal_(model.classifier.weight, mean=0, std=1)\n",
        "#model.classifier.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "iMt9V5iUfQ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
        "std = [0.2673342858792401, 0.2564384629170883, 0.27615047132568404]\n",
        "\n",
        "#test_mean = [0.5088964127604166, 0.48739301317401956, 0.44194221124387256]\n",
        "#test_mean = [0.2682515741720801, 0.2573637364478126, 0.2770957707973042]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(image_size, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)])\n",
        "\n",
        "train_dataset = CIFAR100(root = './', train = True, transform = train_transform, download = True)\n",
        "testTrain_dataset = CIFAR100(root = './', train = True, transform = test_transform, download = True)\n",
        "test_dataset = CIFAR100(root = './', train = False, transform = test_transform, download = True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "testTrain_loader = DataLoader(testTrain_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader_dict = {'Train': testTrain_loader, 'Test': test_loader}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KJRxPa-vyr0",
        "outputId": "2b11707e-714e-46e7-b2fc-4a6fc52f8699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = []\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params.append(param)\n",
        "        \n",
        "optimizer = optim.SGD([\n",
        "        {'params':  params[:-2], 'lr':1.0*lr},\n",
        "        {'params':  params[-1], 'lr':10.0*lr}\n",
        "    ], lr=lr, momentum=momentum)\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.95)"
      ],
      "metadata": {
        "id": "N9HtcvqESbDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, device):\n",
        "  model.train()\n",
        "\n",
        "  for ite, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs).logits\n",
        "\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, test_loader_dict, phase, device):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader_dict[phase]:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs).logits\n",
        "      test_loss += F.cross_entropy(outputs, labels, size_average=False).item()\n",
        "\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      correct += torch.sum(preds == labels.data)\n",
        "      \n",
        "    test_loss /= len(test_loader_dict[phase].dataset)\n",
        "    test_acc = correct / len(test_loader_dict[phase].dataset)\n",
        "\n",
        "    print(f'{phase} loss : {test_loss:.4f} acc : {test_acc:.4f}')\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "VHKZlHYE1UuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUが使用できるかを確認\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "history = {}\n",
        "history['train_loss'] = []\n",
        "history['train_acc'] = []\n",
        "history['test_loss'] = []\n",
        "history['test_acc'] = []\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    print('-----------------------')\n",
        "    print('Epoch {}/{}, lr: {}'.format(epoch,num_epochs, optimizer.param_groups[0][\"lr\"]))\n",
        "\n",
        "    train(model, train_loader, optimizer, device)\n",
        "    val_loss, val_acc = test(model, test_loader_dict, 'Train', device)\n",
        "    test_loss, test_acc = test(model, test_loader_dict, 'Test', device)\n",
        "\n",
        "    scheduler.step() \n",
        "\n",
        "    history['train_loss'].append(val_loss)\n",
        "    history['train_acc'].append(val_acc.to('cpu').item())\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc.to('cpu').item())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p14yneDuafj",
        "outputId": "bff9a89c-b0ff-445b-cdf2-f962f71ddb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "Epoch 1/100, lr: 0.1\n",
            "Train loss: 4.2649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss : 3.8284 acc : 0.1144\n",
            "Test loss : 3.8496 acc : 0.1137\n",
            "-----------------------\n",
            "Epoch 2/100, lr: 0.095\n",
            "Train loss: 3.7326\n",
            "Val loss : 3.3970 acc : 0.1844\n",
            "Test loss : 3.4336 acc : 0.1816\n",
            "-----------------------\n",
            "Epoch 3/100, lr: 0.09025\n",
            "Train loss: 3.3630\n",
            "Val loss : 2.9971 acc : 0.2537\n",
            "Test loss : 3.0361 acc : 0.2509\n",
            "-----------------------\n",
            "Epoch 4/100, lr: 0.0857375\n",
            "Train loss: 3.0746\n",
            "Val loss : 2.6674 acc : 0.3205\n",
            "Test loss : 2.7330 acc : 0.3082\n",
            "-----------------------\n",
            "Epoch 5/100, lr: 0.08145062499999998\n",
            "Train loss: 2.8463\n",
            "Val loss : 2.4399 acc : 0.3673\n",
            "Test loss : 2.5341 acc : 0.3451\n",
            "-----------------------\n",
            "Epoch 6/100, lr: 0.07737809374999999\n",
            "Train loss: 2.6764\n",
            "Val loss : 2.2665 acc : 0.4011\n",
            "Test loss : 2.3708 acc : 0.3765\n",
            "-----------------------\n",
            "Epoch 7/100, lr: 0.07350918906249998\n",
            "Train loss: 2.5231\n",
            "Val loss : 2.1143 acc : 0.4375\n",
            "Test loss : 2.2430 acc : 0.4158\n",
            "-----------------------\n",
            "Epoch 8/100, lr: 0.06983372960937498\n",
            "Train loss: 2.4127\n",
            "Val loss : 1.9927 acc : 0.4611\n",
            "Test loss : 2.1510 acc : 0.4317\n",
            "-----------------------\n",
            "Epoch 9/100, lr: 0.06634204312890622\n",
            "Train loss: 2.3073\n",
            "Val loss : 1.8702 acc : 0.4906\n",
            "Test loss : 2.0381 acc : 0.4519\n",
            "-----------------------\n",
            "Epoch 10/100, lr: 0.0630249409724609\n",
            "Train loss: 2.2200\n",
            "Val loss : 1.7850 acc : 0.5109\n",
            "Test loss : 1.9823 acc : 0.4717\n",
            "-----------------------\n",
            "Epoch 11/100, lr: 0.05987369392383786\n",
            "Train loss: 2.1415\n",
            "Val loss : 1.7190 acc : 0.5248\n",
            "Test loss : 1.9251 acc : 0.4842\n",
            "-----------------------\n",
            "Epoch 12/100, lr: 0.05688000922764597\n",
            "Train loss: 2.0598\n",
            "Val loss : 1.6176 acc : 0.5503\n",
            "Test loss : 1.8515 acc : 0.4976\n",
            "-----------------------\n",
            "Epoch 13/100, lr: 0.05403600876626367\n",
            "Train loss: 1.9911\n",
            "Val loss : 1.5698 acc : 0.5609\n",
            "Test loss : 1.8181 acc : 0.5072\n",
            "-----------------------\n",
            "Epoch 14/100, lr: 0.05133420832795048\n",
            "Train loss: 1.9270\n",
            "Val loss : 1.5254 acc : 0.5727\n",
            "Test loss : 1.7890 acc : 0.5179\n",
            "-----------------------\n",
            "Epoch 15/100, lr: 0.04876749791155295\n",
            "Train loss: 1.8855\n",
            "Val loss : 1.4590 acc : 0.5891\n",
            "Test loss : 1.7484 acc : 0.5209\n",
            "-----------------------\n",
            "Epoch 16/100, lr: 0.046329123015975304\n",
            "Train loss: 1.8261\n",
            "Val loss : 1.3860 acc : 0.6099\n",
            "Test loss : 1.6949 acc : 0.5368\n",
            "-----------------------\n",
            "Epoch 17/100, lr: 0.04401266686517654\n",
            "Train loss: 1.7667\n",
            "Val loss : 1.3570 acc : 0.6154\n",
            "Test loss : 1.6792 acc : 0.5417\n",
            "-----------------------\n",
            "Epoch 18/100, lr: 0.04181203352191771\n",
            "Train loss: 1.7318\n",
            "Val loss : 1.2991 acc : 0.6307\n",
            "Test loss : 1.6388 acc : 0.5489\n",
            "-----------------------\n",
            "Epoch 19/100, lr: 0.039721431845821824\n",
            "Train loss: 1.6904\n",
            "Val loss : 1.2458 acc : 0.6441\n",
            "Test loss : 1.6073 acc : 0.5592\n",
            "-----------------------\n",
            "Epoch 20/100, lr: 0.037735360253530734\n",
            "Train loss: 1.6532\n",
            "Val loss : 1.2370 acc : 0.6480\n",
            "Test loss : 1.6096 acc : 0.5630\n",
            "-----------------------\n",
            "Epoch 21/100, lr: 0.035848592240854196\n",
            "Train loss: 1.6146\n",
            "Val loss : 1.1699 acc : 0.6621\n",
            "Test loss : 1.5788 acc : 0.5668\n",
            "-----------------------\n",
            "Epoch 22/100, lr: 0.03405616262881148\n",
            "Train loss: 1.5720\n",
            "Val loss : 1.1480 acc : 0.6712\n",
            "Test loss : 1.5610 acc : 0.5767\n",
            "-----------------------\n",
            "Epoch 23/100, lr: 0.03235335449737091\n",
            "Train loss: 1.5391\n",
            "Val loss : 1.1083 acc : 0.6799\n",
            "Test loss : 1.5440 acc : 0.5771\n",
            "-----------------------\n",
            "Epoch 24/100, lr: 0.030735686772502362\n",
            "Train loss: 1.5086\n",
            "Val loss : 1.0825 acc : 0.6864\n",
            "Test loss : 1.5287 acc : 0.5771\n",
            "-----------------------\n",
            "Epoch 25/100, lr: 0.029198902433877242\n",
            "Train loss: 1.4814\n",
            "Val loss : 1.0288 acc : 0.7038\n",
            "Test loss : 1.5159 acc : 0.5911\n",
            "-----------------------\n",
            "Epoch 26/100, lr: 0.027738957312183378\n",
            "Train loss: 1.4421\n",
            "Val loss : 1.0178 acc : 0.7052\n",
            "Test loss : 1.5111 acc : 0.5846\n",
            "-----------------------\n",
            "Epoch 27/100, lr: 0.026352009446574207\n",
            "Train loss: 1.4139\n",
            "Val loss : 0.9786 acc : 0.7166\n",
            "Test loss : 1.4868 acc : 0.5935\n",
            "-----------------------\n",
            "Epoch 28/100, lr: 0.025034408974245494\n",
            "Train loss: 1.3864\n",
            "Val loss : 0.9652 acc : 0.7205\n",
            "Test loss : 1.4952 acc : 0.5921\n",
            "-----------------------\n",
            "Epoch 29/100, lr: 0.023782688525533217\n",
            "Train loss: 1.3606\n",
            "Val loss : 0.9291 acc : 0.7303\n",
            "Test loss : 1.4738 acc : 0.5954\n",
            "-----------------------\n",
            "Epoch 30/100, lr: 0.022593554099256556\n",
            "Train loss: 1.3383\n",
            "Val loss : 0.9045 acc : 0.7360\n",
            "Test loss : 1.4722 acc : 0.5998\n",
            "-----------------------\n",
            "Epoch 31/100, lr: 0.021463876394293726\n",
            "Train loss: 1.3161\n",
            "Val loss : 0.8795 acc : 0.7428\n",
            "Test loss : 1.4694 acc : 0.6051\n",
            "-----------------------\n",
            "Epoch 32/100, lr: 0.020390682574579037\n",
            "Train loss: 1.2866\n",
            "Val loss : 0.8628 acc : 0.7486\n",
            "Test loss : 1.4521 acc : 0.6034\n",
            "-----------------------\n",
            "Epoch 33/100, lr: 0.019371148445850084\n",
            "Train loss: 1.2642\n",
            "Val loss : 0.8243 acc : 0.7590\n",
            "Test loss : 1.4369 acc : 0.6080\n",
            "-----------------------\n",
            "Epoch 34/100, lr: 0.01840259102355758\n",
            "Train loss: 1.2401\n",
            "Val loss : 0.8265 acc : 0.7575\n",
            "Test loss : 1.4593 acc : 0.6049\n",
            "-----------------------\n",
            "Epoch 35/100, lr: 0.0174824614723797\n",
            "Train loss: 1.2217\n",
            "Val loss : 0.8013 acc : 0.7642\n",
            "Test loss : 1.4457 acc : 0.6104\n",
            "-----------------------\n",
            "Epoch 36/100, lr: 0.016608338398760712\n",
            "Train loss: 1.2017\n",
            "Val loss : 0.7727 acc : 0.7745\n",
            "Test loss : 1.4535 acc : 0.6097\n",
            "-----------------------\n",
            "Epoch 37/100, lr: 0.015777921478822676\n",
            "Train loss: 1.1798\n",
            "Val loss : 0.7621 acc : 0.7770\n",
            "Test loss : 1.4451 acc : 0.6069\n",
            "-----------------------\n",
            "Epoch 38/100, lr: 0.014989025404881541\n",
            "Train loss: 1.1638\n",
            "Val loss : 0.7487 acc : 0.7795\n",
            "Test loss : 1.4412 acc : 0.6101\n",
            "-----------------------\n",
            "Epoch 39/100, lr: 0.014239574134637464\n",
            "Train loss: 1.1436\n",
            "Val loss : 0.7154 acc : 0.7902\n",
            "Test loss : 1.4164 acc : 0.6178\n",
            "-----------------------\n",
            "Epoch 40/100, lr: 0.01352759542790559\n",
            "Train loss: 1.1308\n",
            "Val loss : 0.7078 acc : 0.7926\n",
            "Test loss : 1.4203 acc : 0.6170\n",
            "-----------------------\n",
            "Epoch 41/100, lr: 0.012851215656510309\n",
            "Train loss: 1.1164\n",
            "Val loss : 0.6924 acc : 0.7978\n",
            "Test loss : 1.4194 acc : 0.6138\n",
            "-----------------------\n",
            "Epoch 42/100, lr: 0.012208654873684792\n",
            "Train loss: 1.0925\n",
            "Val loss : 0.6764 acc : 0.8016\n",
            "Test loss : 1.4233 acc : 0.6149\n",
            "-----------------------\n",
            "Epoch 43/100, lr: 0.011598222130000552\n",
            "Train loss: 1.0764\n",
            "Val loss : 0.6705 acc : 0.8030\n",
            "Test loss : 1.4200 acc : 0.6187\n",
            "-----------------------\n",
            "Epoch 44/100, lr: 0.011018311023500524\n",
            "Train loss: 1.0639\n",
            "Val loss : 0.6447 acc : 0.8105\n",
            "Test loss : 1.4106 acc : 0.6215\n",
            "-----------------------\n",
            "Epoch 45/100, lr: 0.010467395472325497\n",
            "Train loss: 1.0437\n",
            "Val loss : 0.6465 acc : 0.8114\n",
            "Test loss : 1.4174 acc : 0.6214\n",
            "-----------------------\n",
            "Epoch 46/100, lr: 0.009944025698709221\n",
            "Train loss: 1.0369\n",
            "Val loss : 0.6245 acc : 0.8167\n",
            "Test loss : 1.4188 acc : 0.6179\n",
            "-----------------------\n",
            "Epoch 47/100, lr: 0.00944682441377376\n",
            "Train loss: 1.0219\n",
            "Val loss : 0.6052 acc : 0.8233\n",
            "Test loss : 1.4096 acc : 0.6248\n",
            "-----------------------\n",
            "Epoch 48/100, lr: 0.00897448319308507\n",
            "Train loss: 1.0183\n",
            "Val loss : 0.5977 acc : 0.8264\n",
            "Test loss : 1.4013 acc : 0.6219\n",
            "-----------------------\n",
            "Epoch 49/100, lr: 0.008525759033430816\n",
            "Train loss: 0.9997\n",
            "Val loss : 0.5896 acc : 0.8262\n",
            "Test loss : 1.4086 acc : 0.6243\n",
            "-----------------------\n",
            "Epoch 50/100, lr: 0.008099471081759275\n",
            "Train loss: 0.9862\n",
            "Val loss : 0.5787 acc : 0.8307\n",
            "Test loss : 1.4199 acc : 0.6221\n",
            "-----------------------\n",
            "Epoch 51/100, lr: 0.007694497527671311\n",
            "Train loss: 0.9798\n",
            "Val loss : 0.5667 acc : 0.8351\n",
            "Test loss : 1.4053 acc : 0.6278\n",
            "-----------------------\n",
            "Epoch 52/100, lr: 0.007309772651287745\n",
            "Train loss: 0.9661\n",
            "Val loss : 0.5603 acc : 0.8357\n",
            "Test loss : 1.4160 acc : 0.6246\n",
            "-----------------------\n",
            "Epoch 53/100, lr: 0.006944284018723357\n",
            "Train loss: 0.9578\n",
            "Val loss : 0.5512 acc : 0.8410\n",
            "Test loss : 1.4110 acc : 0.6280\n",
            "-----------------------\n",
            "Epoch 54/100, lr: 0.006597069817787189\n",
            "Train loss: 0.9482\n",
            "Val loss : 0.5372 acc : 0.8438\n",
            "Test loss : 1.4145 acc : 0.6233\n",
            "-----------------------\n",
            "Epoch 55/100, lr: 0.006267216326897829\n",
            "Train loss: 0.9391\n",
            "Val loss : 0.5292 acc : 0.8456\n",
            "Test loss : 1.4143 acc : 0.6261\n",
            "-----------------------\n",
            "Epoch 56/100, lr: 0.005953855510552938\n",
            "Train loss: 0.9265\n",
            "Val loss : 0.5199 acc : 0.8490\n",
            "Test loss : 1.4002 acc : 0.6279\n",
            "-----------------------\n",
            "Epoch 57/100, lr: 0.005656162735025291\n",
            "Train loss: 0.9184\n",
            "Val loss : 0.5157 acc : 0.8507\n",
            "Test loss : 1.4108 acc : 0.6275\n",
            "-----------------------\n",
            "Epoch 58/100, lr: 0.005373354598274026\n",
            "Train loss: 0.9140\n",
            "Val loss : 0.5096 acc : 0.8510\n",
            "Test loss : 1.4179 acc : 0.6279\n",
            "-----------------------\n",
            "Epoch 59/100, lr: 0.005104686868360324\n",
            "Train loss: 0.9053\n",
            "Val loss : 0.5013 acc : 0.8535\n",
            "Test loss : 1.4145 acc : 0.6291\n",
            "-----------------------\n",
            "Epoch 60/100, lr: 0.004849452524942308\n",
            "Train loss: 0.8986\n",
            "Val loss : 0.4947 acc : 0.8560\n",
            "Test loss : 1.4195 acc : 0.6264\n",
            "-----------------------\n",
            "Epoch 61/100, lr: 0.004606979898695193\n",
            "Train loss: 0.8998\n",
            "Val loss : 0.4829 acc : 0.8591\n",
            "Test loss : 1.4158 acc : 0.6268\n",
            "-----------------------\n",
            "Epoch 62/100, lr: 0.004376630903760433\n",
            "Train loss: 0.8793\n",
            "Val loss : 0.4815 acc : 0.8608\n",
            "Test loss : 1.4228 acc : 0.6294\n",
            "-----------------------\n",
            "Epoch 63/100, lr: 0.0041577993585724116\n",
            "Train loss: 0.8770\n",
            "Val loss : 0.4782 acc : 0.8621\n",
            "Test loss : 1.4234 acc : 0.6250\n",
            "-----------------------\n",
            "Epoch 64/100, lr: 0.0039499093906437905\n",
            "Train loss: 0.8626\n",
            "Val loss : 0.4782 acc : 0.8614\n",
            "Test loss : 1.4252 acc : 0.6277\n",
            "-----------------------\n",
            "Epoch 65/100, lr: 0.0037524139211116006\n",
            "Train loss: 0.8625\n",
            "Val loss : 0.4662 acc : 0.8660\n",
            "Test loss : 1.4221 acc : 0.6291\n",
            "-----------------------\n",
            "Epoch 66/100, lr: 0.0035647932250560204\n",
            "Train loss: 0.8556\n",
            "Val loss : 0.4588 acc : 0.8686\n",
            "Test loss : 1.4160 acc : 0.6295\n",
            "-----------------------\n",
            "Epoch 67/100, lr: 0.003386553563803219\n",
            "Train loss: 0.8549\n",
            "Val loss : 0.4526 acc : 0.8690\n",
            "Test loss : 1.4235 acc : 0.6268\n",
            "-----------------------\n",
            "Epoch 68/100, lr: 0.003217225885613058\n",
            "Train loss: 0.8525\n",
            "Val loss : 0.4536 acc : 0.8684\n",
            "Test loss : 1.4242 acc : 0.6250\n",
            "-----------------------\n",
            "Epoch 69/100, lr: 0.0030563645913324047\n",
            "Train loss: 0.8469\n",
            "Val loss : 0.4450 acc : 0.8729\n",
            "Test loss : 1.4158 acc : 0.6271\n",
            "-----------------------\n",
            "Epoch 70/100, lr: 0.0029035463617657843\n",
            "Train loss: 0.8401\n",
            "Val loss : 0.4413 acc : 0.8734\n",
            "Test loss : 1.4230 acc : 0.6314\n",
            "-----------------------\n",
            "Epoch 71/100, lr: 0.002758369043677495\n",
            "Train loss: 0.8342\n",
            "Val loss : 0.4378 acc : 0.8758\n",
            "Test loss : 1.4173 acc : 0.6300\n",
            "-----------------------\n",
            "Epoch 72/100, lr: 0.00262045059149362\n",
            "Train loss: 0.8299\n",
            "Val loss : 0.4355 acc : 0.8759\n",
            "Test loss : 1.4231 acc : 0.6305\n",
            "-----------------------\n",
            "Epoch 73/100, lr: 0.002489428061918939\n",
            "Train loss: 0.8191\n",
            "Val loss : 0.4279 acc : 0.8782\n",
            "Test loss : 1.4148 acc : 0.6339\n",
            "-----------------------\n",
            "Epoch 74/100, lr: 0.002364956658822992\n",
            "Train loss: 0.8167\n",
            "Val loss : 0.4265 acc : 0.8788\n",
            "Test loss : 1.4215 acc : 0.6316\n",
            "-----------------------\n",
            "Epoch 75/100, lr: 0.002246708825881842\n",
            "Train loss: 0.8180\n",
            "Val loss : 0.4265 acc : 0.8782\n",
            "Test loss : 1.4259 acc : 0.6330\n",
            "-----------------------\n",
            "Epoch 76/100, lr: 0.00213437338458775\n",
            "Train loss: 0.8126\n",
            "Val loss : 0.4226 acc : 0.8794\n",
            "Test loss : 1.4325 acc : 0.6291\n",
            "-----------------------\n",
            "Epoch 77/100, lr: 0.0020276547153583622\n",
            "Train loss: 0.8053\n",
            "Val loss : 0.4186 acc : 0.8797\n",
            "Test loss : 1.4277 acc : 0.6305\n",
            "-----------------------\n",
            "Epoch 78/100, lr: 0.001926271979590444\n",
            "Train loss: 0.8035\n",
            "Val loss : 0.4155 acc : 0.8817\n",
            "Test loss : 1.4297 acc : 0.6301\n",
            "-----------------------\n",
            "Epoch 79/100, lr: 0.0018299583806109217\n",
            "Train loss: 0.8075\n",
            "Val loss : 0.4165 acc : 0.8801\n",
            "Test loss : 1.4333 acc : 0.6333\n",
            "-----------------------\n",
            "Epoch 80/100, lr: 0.0017384604615803755\n",
            "Train loss: 0.7968\n",
            "Val loss : 0.4052 acc : 0.8854\n",
            "Test loss : 1.4199 acc : 0.6332\n",
            "-----------------------\n",
            "Epoch 81/100, lr: 0.0016515374385013568\n",
            "Train loss: 0.7958\n",
            "Val loss : 0.4041 acc : 0.8852\n",
            "Test loss : 1.4248 acc : 0.6325\n",
            "-----------------------\n",
            "Epoch 82/100, lr: 0.0015689605665762888\n",
            "Train loss: 0.7929\n",
            "Val loss : 0.4051 acc : 0.8833\n",
            "Test loss : 1.4280 acc : 0.6308\n",
            "-----------------------\n",
            "Epoch 83/100, lr: 0.0014905125382474742\n",
            "Train loss: 0.7830\n",
            "Val loss : 0.4037 acc : 0.8855\n",
            "Test loss : 1.4308 acc : 0.6309\n",
            "-----------------------\n",
            "Epoch 84/100, lr: 0.0014159869113351004\n",
            "Train loss: 0.7935\n",
            "Val loss : 0.4107 acc : 0.8819\n",
            "Test loss : 1.4352 acc : 0.6294\n",
            "-----------------------\n",
            "Epoch 85/100, lr: 0.0013451875657683454\n",
            "Train loss: 0.7789\n",
            "Val loss : 0.3960 acc : 0.8864\n",
            "Test loss : 1.4301 acc : 0.6325\n",
            "-----------------------\n",
            "Epoch 86/100, lr: 0.001277928187479928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "\n",
        "def plot_graph(values1, values2, rng, label1, label2):\n",
        "    plt.plot(range(rng), values1, label=label1)\n",
        "    plt.plot(range(rng), values2, label=label2)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()  "
      ],
      "metadata": {
        "id": "Kyf6Aza4sRB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history['train_acc'], history['test_acc'], num_epochs,'train_acc', 'test_acc')"
      ],
      "metadata": {
        "id": "lMKX3Sc7sSAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history['train_loss'], history['test_loss'], num_epochs,'train_loss', 'test_loss')"
      ],
      "metadata": {
        "id": "gtp91Tt6VowB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}